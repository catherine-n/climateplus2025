{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e1fb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 877 PV-related arrays\n",
      "Saved filtered annotations to final_annotations_PV_all_types.gpkg\n",
      "Remaining columns: ['id', 'PV_normal', 'PV_heater', 'PV_pool', 'area', 'annotator', 'centroid_latitude', 'centroid_longitude', 'image_name', 'nw_corner_of_image_latitude', 'nw_corner_of_image_longitude', 'se_corner_of_image_latitude', 'se_corner_of_image_longitude', 'PV_normal_qc', 'PV_heater_qc', 'PV_pool_qc', 'PV_heater_mat_qc', 'uncertflag_qc', 'delete_qc', 'resizing_qc', 'PV_heater_mat_combined', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "#gdf = gpd.read_file('/home/cmn60/cape_town_segmentation/final_annotations_PV_all_types_5K_cleaned.gpkg')\n",
    "gdf = gpd.read_file('/home/cmn60/cape_town_segmentation/final_annotations_PV_all_types_balanced_3_cleaned.gpkg')\n",
    "\n",
    "# Keep rows where at least one of the PV flags is 1\n",
    "pv_mask = (gdf['PV_normal'] == 1) | (gdf['PV_heater'] == 1) | (gdf['PV_pool'] == 1)\n",
    "\n",
    "# Exclude rows where uncertflag == 1\n",
    "uncert_mask = gdf['uncertflag'] != 1\n",
    "# Exclude rows where both PV_heater and PV_pool are 1, only 2 observations\n",
    "heater_pool_overlap_mask = ~((gdf['PV_heater'] == 1) & (gdf['PV_pool'] == 1))\n",
    "\n",
    "\n",
    "filtered_gdf = gdf[pv_mask & uncert_mask & heater_pool_overlap_mask].copy()\n",
    "\n",
    "# Drop the 'uncertflag' column\n",
    "if 'uncertflag' in filtered_gdf.columns:\n",
    "    filtered_gdf = filtered_gdf.drop(columns=['uncertflag'])\n",
    "\n",
    "# Output stats\n",
    "print(f\"Filtered dataset contains {len(filtered_gdf)} PV-related arrays\")\n",
    "\n",
    "# Save the filtered annotations\n",
    "output_path = \"final_annotations_PV_all_types.gpkg\"\n",
    "filtered_gdf.to_file(output_path, driver=\"GPKG\")\n",
    "print(f\"Saved filtered annotations to {output_path}\")\n",
    "\n",
    "# Show final column names\n",
    "print(\"Remaining columns:\", filtered_gdf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d87aa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid geometries found: 0\n",
      "Still invalid after fixing: 0\n"
     ]
    }
   ],
   "source": [
    "# Geometry error checker\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "gdf = gpd.read_file(\"final_annotations_PV_all_types.gpkg\")\n",
    "\n",
    "# Invalid geometry\n",
    "invalid_gdf = gdf[~gdf.geometry.is_valid].copy()\n",
    "invalid_gdf[\"image_name\"] = invalid_gdf[\"image_name\"]\n",
    "print(f\"Invalid geometries found: {len(invalid_gdf)}\")\n",
    "\n",
    "# Error correction : buffer(0) method \n",
    "gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda geom: geom.buffer(0) if not geom.is_valid else geom)\n",
    "\n",
    "\n",
    "still_invalid = gdf[~gdf.geometry.is_valid].copy()\n",
    "print(f\"Still invalid after fixing: {len(still_invalid)}\")\n",
    "\n",
    "gdf_valid = gdf[gdf.geometry.is_valid].copy()\n",
    "gdf_valid.to_file(\"final_annotations_PV_all_types_cleaned.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283a01b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "def convert_gpkg_to_pixel_coords(gpkg_path, image_dir):\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "    pixel_rows = []\n",
    "\n",
    "    for image_name in gdf['image_name'].unique():\n",
    "        image_path = os.path.join(image_dir, f\"{image_name}.tif\")\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        with rasterio.open(image_path) as src:\n",
    "            transform = src.transform\n",
    "            inv_transform = ~transform\n",
    "            matrix = [inv_transform.a, inv_transform.b, inv_transform.d,\n",
    "                      inv_transform.e, inv_transform.xoff, inv_transform.yoff]\n",
    "\n",
    "            image_gdf = gdf[gdf['image_name'] == image_name]\n",
    "            for _, row in image_gdf.iterrows():\n",
    "                geom = row.geometry\n",
    "                if geom.is_empty or geom is None:\n",
    "                    continue\n",
    "\n",
    "                geom_px = affine_transform(geom, matrix)\n",
    "\n",
    "                if isinstance(geom_px, Polygon):\n",
    "                    polygons = [geom_px]\n",
    "                elif isinstance(geom_px, MultiPolygon):\n",
    "                    polygons = list(geom_px.geoms)\n",
    "                else:\n",
    "                    continue \n",
    "\n",
    "                for poly in polygons:\n",
    "                    centroid = poly.centroid\n",
    "                    pixel_rows.append({\n",
    "                        'image_name': image_name,\n",
    "                        'geometry': poly,\n",
    "                        'polygon_vertices_pixels': np.array(poly.exterior.coords),\n",
    "                        'centroid_latitude_pixels': centroid.y,\n",
    "                        'centroid_longitude_pixels': centroid.x,\n",
    "                        'PV_normal': row['PV_normal'],\n",
    "                        'PV_heater': row['PV_heater'],\n",
    "                        'PV_pool': row['PV_pool']\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(pixel_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f892b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def create_multiclass_mask(image_shape, polygons_with_classes):\n",
    "    mask = np.zeros(image_shape[:2], dtype=\"uint8\")\n",
    "    for polygon, class_id in polygons_with_classes:\n",
    "        cv2.fillPoly(mask, [polygon.astype(np.int32)], class_id)\n",
    "    return mask\n",
    "\n",
    "def adjust_polygon_coordinates(polygons, x_offset, y_offset):\n",
    "    return [(poly - np.array([x_offset, y_offset]), cls) for poly, cls in polygons]\n",
    "\n",
    "def save_tile_and_mask(tile, mask, tile_index_pixels, tile_dir, mask_dir, image_name):\n",
    "    tile_path = os.path.join(tile_dir, f\"i_{image_name}_{tile_index_pixels}.png\")\n",
    "    mask_path = os.path.join(mask_dir, f\"m_{image_name}_{tile_index_pixels}.png\")\n",
    "    cv2.imwrite(tile_path, cv2.cvtColor(tile, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(mask_path, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afb6c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geotiff(image_name, image_path, tile_size, df, tile_dir, mask_dir):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        img = np.transpose(src.read(), (1, 2, 0))\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        pad_h = (tile_size - h % tile_size) % tile_size\n",
    "        pad_w = (tile_size - w % tile_size) % tile_size\n",
    "        padded = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "\n",
    "        for y in range(0, padded.shape[0], tile_size):\n",
    "            for x in range(0, padded.shape[1], tile_size):\n",
    "                tile = padded[y:y+tile_size, x:x+tile_size]\n",
    "                polygons_in_tile = []\n",
    "\n",
    "                for _, row in df.iterrows():\n",
    "                    cx, cy = row['centroid_longitude_pixels'], row['centroid_latitude_pixels']\n",
    "                    if x <= cx < x + tile_size and y <= cy < y + tile_size:\n",
    "                        poly = row['polygon_vertices_pixels']\n",
    "                        if row['PV_normal'] == 1:\n",
    "                            cls = 1\n",
    "                        elif row['PV_heater'] == 1:\n",
    "                            cls = 2\n",
    "                        elif row['PV_pool'] == 1:\n",
    "                            cls = 3\n",
    "                        else:\n",
    "                            continue\n",
    "                        polygons_in_tile.append((poly, cls))\n",
    "\n",
    "                adj_polygons = adjust_polygon_coordinates(polygons_in_tile, x, y)\n",
    "                mask = create_multiclass_mask(tile.shape, adj_polygons)\n",
    "\n",
    "                if np.any(mask > 0):\n",
    "                    tile_idx = f\"{y//tile_size}_{x//tile_size}\"\n",
    "                    save_tile_and_mask(tile, mask, tile_idx, tile_dir, mask_dir, image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a723b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_images(image_dir, annotations_df, tile_size, tile_dir, mask_dir):\n",
    "    os.makedirs(tile_dir, exist_ok=True)\n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "    for i, image_name in enumerate(annotations_df['image_name'].unique()):\n",
    "        print(f\"[{i}] Processing {image_name}\")\n",
    "        img_path = os.path.join(image_dir, f\"{image_name}.tif\")\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        df_img = annotations_df[annotations_df['image_name'] == image_name]\n",
    "        process_geotiff(image_name, img_path, tile_size, df_img, tile_dir, mask_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54acb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_masks_with_target(src_folder, dst_folder):\n",
    "    os.makedirs(dst_folder, exist_ok=True)\n",
    "    for f in os.listdir(src_folder):\n",
    "        mask = cv2.imread(os.path.join(src_folder, f), cv2.IMREAD_GRAYSCALE)\n",
    "        if np.any(mask > 0):\n",
    "            shutil.copy(os.path.join(src_folder, f), dst_folder)\n",
    "\n",
    "def copy_corresponding_images(mask_folder, image_folder, dst_folder):\n",
    "    os.makedirs(dst_folder, exist_ok=True)\n",
    "    for f in os.listdir(mask_folder):\n",
    "        img_f = \"i\" + f[1:]\n",
    "        src = os.path.join(image_folder, img_f)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c768d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Processing 2023_RGB_8cm_W57B_8\n",
      "[1] Processing 2023_RGB_8cm_W24A_17\n",
      "[2] Processing 2023_RGB_8cm_W25C_16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from shapely.affinity import affine_transform\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "gpkg_path = \"final_annotations_PV_all_types_cleaned.gpkg\"\n",
    "image_dir = \"/home/il72/cape_town_annotation_checker/1.db_pipeline/download/images\"\n",
    "tile_dir = \"tiles_320_1k_new\"\n",
    "mask_dir = \"masks_320_1k_new\"\n",
    "target_mask_dir = \"masks_target_1k_new\"\n",
    "target_image_dir = \"images_target_1k_new\"\n",
    "tile_size = 320\n",
    "\n",
    "annotations_df = convert_gpkg_to_pixel_coords(gpkg_path, image_dir)\n",
    "\n",
    "process_all_images(image_dir, annotations_df, tile_size, tile_dir, mask_dir)\n",
    "\n",
    "copy_masks_with_target(mask_dir, target_mask_dir)\n",
    "copy_corresponding_images(target_mask_dir, tile_dir, target_image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beaf68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset split completed.\n"
     ]
    }
   ],
   "source": [
    "# Unstratified dataset split: train/val/test (see below for stratified)\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Set your input directories\n",
    "images_dir = '/home/cmn60/cape_town_segmentation/images_target_1k_new'\n",
    "masks_dir = '/home/cmn60/cape_town_segmentation/masks_target_1k_new'\n",
    "\n",
    "# Set your output base directory\n",
    "output_dir = '/home/cmn60/cape_town_segmentation/output1k_new'\n",
    "\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.tif'))]\n",
    "image_suffixes = [f[2:] for f in image_files]  # Remove 'i_' prefix\n",
    "\n",
    "# Shuffle\n",
    "random.shuffle(image_suffixes)\n",
    "\n",
    "# Split\n",
    "total = len(image_suffixes)\n",
    "train_end = int(0.7 * total)\n",
    "val_end = train_end + int(0.15 * total)\n",
    "\n",
    "splits = {\n",
    "    'train': image_suffixes[:train_end],\n",
    "    'val': image_suffixes[train_end:val_end],\n",
    "    'test': image_suffixes[val_end:]\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(output_dir, split, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, split, 'masks'), exist_ok=True)\n",
    "\n",
    "# Copy files\n",
    "for split_name, suffix_list in splits.items():\n",
    "    for suffix in suffix_list:\n",
    "        image_file = f\"i_{suffix}\"\n",
    "        mask_file = f\"m_{suffix}\"\n",
    "        \n",
    "        src_image = os.path.join(images_dir, image_file)\n",
    "        src_mask = os.path.join(masks_dir, mask_file)\n",
    "        dst_image = os.path.join(output_dir, split_name, 'images', image_file)\n",
    "        dst_mask = os.path.join(output_dir, split_name, 'masks', mask_file)\n",
    "        \n",
    "        if os.path.exists(src_image) and os.path.exists(src_mask):\n",
    "            shutil.copy(src_image, dst_image)\n",
    "            shutil.copy(src_mask, dst_mask)\n",
    "        else:\n",
    "            print(f\"Warning: Missing pair for {suffix}\")\n",
    "\n",
    "print(\"✅ Dataset split completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f40183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified dataset split\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "images_dir = '/home/cmn60/cape_town_segmentation/images_target_1k_new'\n",
    "masks_dir = '/home/cmn60/cape_town_segmentation/masks_target_1k_new'\n",
    "output_dir = '/home/cmn60/cape_town_segmentation/output1k_new_stratified'\n",
    "\n",
    "# Step 1: Load all mask files and assign class combination label\n",
    "data = []\n",
    "for fname in os.listdir(masks_dir):\n",
    "    if fname.endswith('.png'):\n",
    "        mask = cv2.imread(os.path.join(masks_dir, fname), cv2.IMREAD_GRAYSCALE)\n",
    "        classes = set(np.unique(mask)) - {0}  # exclude background\n",
    "\n",
    "        combo = f\"{int(1 in classes)}_{int(2 in classes)}_{int(3 in classes)}\"\n",
    "        suffix = fname[2:]  # strip 'm_' prefix\n",
    "        data.append({'suffix': suffix, 'combo': combo})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Stratified split\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.10, stratify=df['combo'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1111, stratify=train_val_df['combo'], random_state=42)\n",
    "\n",
    "splits = {\n",
    "    'train': train_df['suffix'].tolist(),\n",
    "    'val': val_df['suffix'].tolist(),\n",
    "    'test': test_df['suffix'].tolist()\n",
    "}\n",
    "\n",
    "# Step 3: Copy files to output structure\n",
    "import shutil\n",
    "\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(output_dir, split, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, split, 'masks'), exist_ok=True)\n",
    "\n",
    "for split_name, suffixes in splits.items():\n",
    "    for suffix in suffixes:\n",
    "        img_file = f\"i_{suffix}\"\n",
    "        mask_file = f\"m_{suffix}\"\n",
    "\n",
    "        src_img = os.path.join(images_dir, img_file)\n",
    "        src_mask = os.path.join(masks_dir, mask_file)\n",
    "        dst_img = os.path.join(output_dir, split_name, 'images', img_file)\n",
    "        dst_mask = os.path.join(output_dir, split_name, 'masks', mask_file)\n",
    "\n",
    "        if os.path.exists(src_img) and os.path.exists(src_mask):\n",
    "            shutil.copy(src_img, dst_img)\n",
    "            shutil.copy(src_mask, dst_mask)\n",
    "        else:\n",
    "            print(f\"⚠️ Missing pair for {suffix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06961d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 TRAIN combo distribution:\n",
      "combo\n",
      "0_1_0    132\n",
      "0_0_1    107\n",
      "1_0_0     83\n",
      "0_1_1     20\n",
      "1_1_0     20\n",
      "1_0_1     17\n",
      "1_1_1      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔹 VAL combo distribution:\n",
      "combo\n",
      "0_1_0    17\n",
      "0_0_1    14\n",
      "1_0_0    10\n",
      "1_1_0     3\n",
      "1_0_1     2\n",
      "0_1_1     2\n",
      "1_1_1     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔹 TEST combo distribution:\n",
      "combo\n",
      "0_1_0    17\n",
      "0_0_1    14\n",
      "1_0_0    10\n",
      "0_1_1     3\n",
      "1_1_0     2\n",
      "1_0_1     2\n",
      "1_1_1     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print dataset splitting\n",
    "# 0_1_0 means the image has (no) PV normal, (yes) PV heater, and (no) PV pool\n",
    "for split_name, suffix_list in splits.items():\n",
    "    split_df = df[df['suffix'].isin(suffix_list)]\n",
    "    print(f\"\\n🔹 {split_name.upper()} combo distribution:\")\n",
    "    print(split_df['combo'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b9a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PV_normal: 334\n",
      "PV_heater: 307\n",
      "PV_pool: 236\n"
     ]
    }
   ],
   "source": [
    "# Count and print number of annotations for each PV type\n",
    "num_normal = (gdf_valid['PV_normal'] == 1).sum()\n",
    "num_heater = (gdf_valid['PV_heater'] == 1).sum()\n",
    "num_pool = (gdf_valid['PV_pool'] == 1).sum()\n",
    "\n",
    "print(f\"PV_normal: {num_normal}\")\n",
    "print(f\"PV_heater: {num_heater}\")\n",
    "print(f\"PV_pool: {num_pool}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
