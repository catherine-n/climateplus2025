{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5254d56",
   "metadata": {},
   "source": [
    "There are two evaluation methods: !!FOR POSTER WE USE Method2.!! <br>\n",
    "\n",
    "[1] **Method1. Direct Image Comparison** <br>\n",
    "The predicted image is reconstructed to 12500×12500 size, then re-split into 320×320 tiles to compare directly with ground truth.\n",
    "\n",
    "- This script reconstructs full-size segmentation masks (12500 * 12500) from U-Net predictions generated from 320×320 crops centered on YOLO-detected centroids. \n",
    "\n",
    "- Read YOLO detection results and tile metadata(csv), which are used to map each local centroid back to global image coordinates. For each centroid, the corresponding U-Net prediction (stored in a .json file) is loaded and correctly aligned within the full-size canvas, accounting for tile offsets and boundary clipping. \n",
    "\n",
    "- To ensure accurate merging, only valid predicted regions within tile boundaries are pasted, and overlapping regions are resolved using np.maximum() — retaining the highest class index when overlaps occur. <br>\n",
    "\n",
    "- **Note**\n",
    "The class index ordering(based on Precision per class) is:\n",
    "    - 0 = background, 1 = solar panel, 2 = pool heater, 3 = water heater.\n",
    "- The output is a single .png mask per base image(12500 * 12500 size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fba94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO CSV and tile metadata...\n",
      "\n",
      "▶ Reconstructing mask for: 2023_RGB_8cm_W24A_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/390 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [01:12<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /shared/data/climateplus2025/YOLO+U-Net_Prediction_updated_0722/reconstructed_prediction_masks_12500/reconstructed_mask_2023_RGB_8cm_W24A_17.png\n",
      "\n",
      "▶ Reconstructing mask for: 2023_RGB_8cm_W25C_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:42<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /shared/data/climateplus2025/YOLO+U-Net_Prediction_updated_0722/reconstructed_prediction_masks_12500/reconstructed_mask_2023_RGB_8cm_W25C_16.png\n",
      "\n",
      "▶ Reconstructing mask for: 2023_RGB_8cm_W57B_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 242/242 [00:54<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /shared/data/climateplus2025/YOLO+U-Net_Prediction_updated_0722/reconstructed_prediction_masks_12500/reconstructed_mask_2023_RGB_8cm_W57B_8.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================\n",
    "# CONFIGURATION\n",
    "# ============================\n",
    "yolo_csv_path = \"/shared/data/climateplus2025/Prediction_for_poster_3images_July21/processed_centroids_and_bbox.csv\"\n",
    "tile_meta_csv = \"/shared/data/climateplus2025/Prediction_for_poster_3images_July21/tile_metadata_for_reconstruction.csv\"\n",
    "# !!! refer to second code chunk in 0.Data_Processing.ipynb in Prediction_for_poster_July21 folder !!!\n",
    "unet_pred_mask_dir = \"/home/cmn60/cape_town_segmentation/prediction_outputs_v42\" #v40 : U-Net only, # v42 : YOLO+U-Net # 48 : U-Net only(only predicted value)\n",
    "# In Catherine's code there are predicted value and Ground Truth (GT) masks together\n",
    "original_image_shape = (12500, 12500)  # Full image size\n",
    "output_dir = \"/shared/data/climateplus2025/YOLO+U-Net_Prediction_3images_updated_head_to_head_comparision_0722/reconstructed_prediction_masks_12500\"\n",
    "tile_crop_size = 320\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================\n",
    "def load_json_pred_mask(json_path):\n",
    "    \"\"\"\n",
    "    Reads prediction JSON file and returns a 320x320 class-indexed mask,\n",
    "    using ONLY 'predicted_coords'. Ground truth data is ignored.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    pred_coords = data.get(\"predicted_coords\", {})\n",
    "    mask = np.zeros((tile_crop_size, tile_crop_size), dtype=np.uint8)\n",
    "\n",
    "    # Class label mapping — must match your model output ordering\n",
    "    class_order = [\"background\", \"PV_normal\", \"PV_pool\", \"PV_heater\"]\n",
    "\n",
    "    for class_idx, cls_name in enumerate(class_order):\n",
    "        coords = pred_coords.get(cls_name, [])\n",
    "        for y, x in coords:\n",
    "            # Protect against malformed coordinates\n",
    "            if 0 <= y < tile_crop_size and 0 <= x < tile_crop_size:\n",
    "                mask[y, x] = class_idx\n",
    "\n",
    "    return mask\n",
    "\n",
    "# ============================\n",
    "# LOAD CSVs\n",
    "# ============================\n",
    "print(\"Loading YOLO CSV and tile metadata...\")\n",
    "df = pd.read_csv(yolo_csv_path)\n",
    "tile_meta_df = pd.read_csv(tile_meta_csv)\n",
    "tile_meta_df.set_index(\"tile_name\", inplace=True)\n",
    "\n",
    "# Group by base image\n",
    "df[\"base_image_name\"] = df[\"image_name\"].apply(lambda x: x.split(\"_tile_\")[0])\n",
    "grouped = df.groupby(\"base_image_name\")\n",
    "\n",
    "# ============================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ============================\n",
    "for base_image_name, group_df in grouped:\n",
    "    print(f\"\\n▶ Reconstructing mask for: {base_image_name}\")\n",
    "\n",
    "    full_pred_mask = np.zeros(original_image_shape, dtype=np.uint8)\n",
    "\n",
    "    for idx, row in tqdm(group_df.iterrows(), total=len(group_df)):\n",
    "        pred_id = row[\"prediction_id\"]\n",
    "        image_name = row[\"image_name\"]\n",
    "        centroid_str = row[\"pixel_centroid\"]\n",
    "\n",
    "        # 1. Load tile metadata\n",
    "        if image_name not in tile_meta_df.index:\n",
    "            print(f\"[Warning] Missing metadata: {image_name}\")\n",
    "            continue\n",
    "\n",
    "        tile_info = tile_meta_df.loc[image_name]\n",
    "        tile_x = int(tile_info[\"tile_x\"])\n",
    "        tile_y = int(tile_info[\"tile_y\"])\n",
    "        tile_w = int(tile_info[\"tile_width\"])\n",
    "        tile_h = int(tile_info[\"tile_height\"])\n",
    "\n",
    "        # 2. Parse centroid\n",
    "        try:\n",
    "            cx, cy = eval(centroid_str)\n",
    "            cx, cy = int(round(cx)), int(round(cy))\n",
    "        except:\n",
    "            print(f\"[Warning] Invalid centroid at row {idx}\")\n",
    "            continue\n",
    "\n",
    "        # 3. Global coordinates of center\n",
    "        global_cx = tile_x + cx\n",
    "        global_cy = tile_y + cy\n",
    "\n",
    "        # 4. Define crop box in global coordinates\n",
    "        x1 = global_cx - tile_crop_size // 2\n",
    "        y1 = global_cy - tile_crop_size // 2\n",
    "        x2 = x1 + tile_crop_size\n",
    "        y2 = y1 + tile_crop_size\n",
    "\n",
    "        # 5. Clip to full image boundaries\n",
    "        x1_clip = max(0, x1)\n",
    "        y1_clip = max(0, y1)\n",
    "        x2_clip = min(original_image_shape[1], x2)\n",
    "        y2_clip = min(original_image_shape[0], y2)\n",
    "\n",
    "        w = x2_clip - x1_clip\n",
    "        h = y2_clip - y1_clip\n",
    "        if w <= 0 or h <= 0:\n",
    "            continue\n",
    "\n",
    "        x_offset = x1_clip - x1\n",
    "        y_offset = y1_clip - y1\n",
    "\n",
    "        # 6. Load prediction mask\n",
    "        if not pred_id.startswith(\"i_\"):\n",
    "            pred_id = \"i_\" + pred_id\n",
    "\n",
    "        json_path = os.path.join(unet_pred_mask_dir, f\"{pred_id}.json\")\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"[Warning] Missing JSON: {json_path}\")\n",
    "            continue\n",
    "\n",
    "        pred_mask = load_json_pred_mask(json_path)\n",
    "\n",
    "        # 7. Crop only valid region\n",
    "        x_crop_end = min(x_offset + w, tile_w)\n",
    "        y_crop_end = min(y_offset + h, tile_h)\n",
    "        pred_crop = pred_mask[y_offset:y_crop_end, x_offset:x_crop_end]\n",
    "\n",
    "        # 8. Paste into full image\n",
    "        full_pred_mask[y1_clip:y1_clip + pred_crop.shape[0], x1_clip:x1_clip + pred_crop.shape[1]] = np.maximum(\n",
    "            full_pred_mask[y1_clip:y1_clip + pred_crop.shape[0], x1_clip:x1_clip + pred_crop.shape[1]],\n",
    "            pred_crop\n",
    "        )\n",
    "\n",
    "    # Save result\n",
    "    output_path = os.path.join(output_dir, f\"reconstructed_mask_{base_image_name}.png\")\n",
    "    cv2.imwrite(output_path, full_pred_mask)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba509879",
   "metadata": {},
   "source": [
    "The output above is a single large prediction mask image (12,500 × 12,500). However, our ground truth (GT) labels were already generated during the U-Net pipeline as cropped 320 × 320 images.\n",
    "\n",
    "Therefore, we need to crop the large prediction mask into 320 × 320 patches and match them with the corresponding ground truth files.\n",
    "The file names must match those used in the U-Net ground truth mask folder.\n",
    "For example: m_xxx_xxx format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f53c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CONFIGURATION\n",
    "reconstructed_mask_dir = \"/shared/data/climateplus2025/YOLO+U-Net_Prediction_3images_updated_head_to_head_comparision_0722/reconstructed_prediction_masks_12500\"\n",
    "tile_size = 320\n",
    "tile_output_dir = \"/shared/data/climateplus2025/YOLO+U-Net_Prediction_3images_updated_0722/prediction_masks_tiles_320\"\n",
    "\n",
    "os.makedirs(tile_output_dir, exist_ok=True)\n",
    "\n",
    "# LOOP OVER MASKS\n",
    "for filename in tqdm(os.listdir(reconstructed_mask_dir)):\n",
    "    if not filename.endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "    base_name = filename.replace(\"reconstructed_mask_\", \"\").replace(\".png\", \"\")\n",
    "    mask_path = os.path.join(reconstructed_mask_dir, filename)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)  # Read as grayscale or multi-class\n",
    "\n",
    "    height, width = mask.shape[:2]\n",
    "\n",
    "    # Calculate padding (if needed)\n",
    "    pad_h = (tile_size - height % tile_size) % tile_size\n",
    "    pad_w = (tile_size - width % tile_size) % tile_size\n",
    "    padded_mask = np.pad(mask, ((0, pad_h), (0, pad_w)), mode='constant')\n",
    "\n",
    "    padded_height, padded_width = padded_mask.shape\n",
    "\n",
    "    # Tile loop\n",
    "    for y in range(0, padded_height, tile_size):\n",
    "        for x in range(0, padded_width, tile_size):\n",
    "            tile = padded_mask[y:y+tile_size, x:x+tile_size]\n",
    "\n",
    "            # Skip blank masks\n",
    "            if np.all(tile == 0):\n",
    "                continue\n",
    "\n",
    "            row_idx = y // tile_size\n",
    "            col_idx = x // tile_size\n",
    "\n",
    "            out_filename = f\"m_{base_name}_{row_idx}_{col_idx}.png\"\n",
    "            out_path = os.path.join(tile_output_dir, out_filename)\n",
    "\n",
    "            cv2.imwrite(out_path, tile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1ba27",
   "metadata": {},
   "source": [
    "[2] **Method2. JSON Reconstruction and Conflict Resolution** <br>\n",
    "\n",
    "- Designed to support post-processing for predictions from YOLO + U-Net models.\n",
    "- Maps pixel coordinates from image-level JSON files back to the original 12500×12500 image space.\n",
    "- Uses tile_metadata_for_reconstruction.csv to convert local (320×320 tile) coordinates to global positions.\n",
    "- If multiple classes are predicted at the same pixel, the script:\n",
    "    * Logs the conflict in conflicting_pixels.csv.\n",
    "    * Resolves it based on class precision priority: Background -> Normal -> Pool -> Heater.\n",
    "- To reduce file size, only saves predictions for: Normal, Heater, Pool (Background is not included)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c82bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------\n",
    "tile_meta_csv = \"/shared/data/climateplus2025/Prediction_entire_0801/tile_metadata_for_reconstruction.csv\"\n",
    "json_input_dir = \"/shared/data/climateplus2025/Prediction_EntireDataset_YOLO+Unet/U-Net_prediction_output/prediction_outputs_v44\"\n",
    "output_json_dir = \"/shared/data/climateplus2025/Prediction_entire_0801/Merged_prediction_Json/unified_json\"\n",
    "original_image_shape = (12500, 12500)\n",
    "\n",
    "os.makedirs(output_json_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# CLASS PRIORITY (Precision-based)\n",
    "# ----------------------------\n",
    "# Higher index = higher priority\n",
    "class_order = [\"background\", \"PV_normal\", \"PV_heater\", \"PV_pool\"]\n",
    "# Reversed priority: PV_pool > PV_heater > PV_normal > background\n",
    "class_priority = {\"background\": 0, \"PV_normal\": 1, \"PV_heater\": 2, \"PV_pool\": 3}\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD TILE METADATA\n",
    "# ----------------------------\n",
    "tile_meta_df = pd.read_csv(tile_meta_csv)\n",
    "tile_meta_df.set_index(\"tile_name\", inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# OUTPUT STRUCTURES\n",
    "# ----------------------------\n",
    "unified_maps = {}\n",
    "pixel_class_map = defaultdict(set)\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 1: PROCESS JSON FILES\n",
    "# ----------------------------\n",
    "print(\"Processing all prediction JSON files...\")\n",
    "processed_files = 0\n",
    "for fname in tqdm(os.listdir(json_input_dir)):\n",
    "    if not fname.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    pred_id = fname.replace(\".json\", \"\")  # i_2023_RGB_8cm_W24A_17_tile_0_2048_pred_0001\n",
    "    tile_name_from_json = pred_id.replace(\"i_\", \"\").split(\"_pred_\")[0] + \".tif\"\n",
    "\n",
    "    if tile_name_from_json not in tile_meta_df.index:\n",
    "        print(f\"[Warning] Metadata not found for: {tile_name_from_json}\")\n",
    "        continue\n",
    "\n",
    "    tile_info = tile_meta_df.loc[tile_name_from_json]\n",
    "    tile_x = int(tile_info[\"tile_x\"])\n",
    "    tile_y = int(tile_info[\"tile_y\"])\n",
    "\n",
    "    base_image_name = tile_name_from_json.split(\"_tile_\")[0]\n",
    "\n",
    "    json_path = os.path.join(json_input_dir, fname)\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    pred_coords = data.get(\"predicted_coords\", {})\n",
    "\n",
    "    if base_image_name not in unified_maps:\n",
    "        unified_maps[base_image_name] = np.full(original_image_shape, fill_value=0, dtype=np.uint8)\n",
    "\n",
    "    priority_map = unified_maps[base_image_name]\n",
    "\n",
    "    for cls_name, coords in pred_coords.items():\n",
    "        cls_idx = class_priority[cls_name]\n",
    "        for y_local, x_local in coords:\n",
    "            global_y = tile_y + y_local\n",
    "            global_x = tile_x + x_local\n",
    "\n",
    "            if 0 <= global_y < original_image_shape[0] and 0 <= global_x < original_image_shape[1]:\n",
    "                key = (base_image_name, global_y, global_x)\n",
    "                pixel_class_map[key].add(cls_name)\n",
    "\n",
    "                # Conflict resolution using precision-based priority\n",
    "                existing_idx = priority_map[global_y, global_x]\n",
    "                if cls_idx > existing_idx:\n",
    "                    priority_map[global_y, global_x] = cls_idx\n",
    "\n",
    "    processed_files += 1\n",
    "\n",
    "print(f\"\\nProcessed {processed_files} prediction JSON files.\")\n",
    "print(f\"Generated unified maps for {len(unified_maps)} base images.\")\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 2: SAVE CONFLICTING PIXELS\n",
    "# ----------------------------\n",
    "conflicts = {k: v for k, v in pixel_class_map.items() if len(v) > 1}\n",
    "conflict_path = os.path.join(output_json_dir, \"conflicting_pixels.csv\")\n",
    "\n",
    "print(f\"\\nFound {len(conflicts)} conflicting pixels.\")\n",
    "with open(conflict_path, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"base_image_name\", \"y\", \"x\", \"classes\"])\n",
    "    for (img, y, x), cls_set in conflicts.items():\n",
    "        writer.writerow([img, y, x, \",\".join(sorted(cls_set))])\n",
    "\n",
    "print(f\"Conflict list saved to: {conflict_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# STEP 3: SAVE UNIFIED JSONS\n",
    "# ----------------------------\n",
    "print(\"\\nSaving unified JSON files (global coordinates)...\")\n",
    "for base_image_name, class_map in unified_maps.items():\n",
    "    # background 제외\n",
    "    output_coords = {cls: [] for cls in class_order if cls != \"background\"}\n",
    "\n",
    "    for y in range(original_image_shape[0]):\n",
    "        for x in range(original_image_shape[1]):\n",
    "            class_idx = class_map[y, x]\n",
    "            class_name = class_order[class_idx]\n",
    "            if class_name == \"background\":\n",
    "                continue\n",
    "            output_coords[class_name].append([y, x])\n",
    "\n",
    "    out_json = {\"predicted_coords\": output_coords}\n",
    "    out_path = os.path.join(output_json_dir, f\"{base_image_name}.json\")\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(out_json, f)\n",
    "\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "print(\"\\nAll done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
